% !TeX root = ../sustechthesis-example.tex

\begin{conclusion}

本文围绕大语言模型推理服务中的资源管理核心挑战，从批处理调度与部署配置联合优化、PD分离架构下的动态资源协同两个互补维度展开研究，提出了UELLM和BanaServe两个系统框架，取得了以下主要研究成果：
\begin{enumerate}

\item 揭示了LLM推理服务中批处理策略与部署配置的多层次瓶颈，并建立了联合优化框架UELLM。针对现有LLM推理系统中SLO感知缺失、输出长度预测精度不足和部署配置搜索低效等问题，提出了由资源画像器、批处理调度器和LLM部署器三个组件协同构成的UELLM框架。其中，基于LoRA微调ChatGLM3-6B的输出长度预测方案在分桶准确率上达到99.51\%，显著优于基于轻量级分类器的现有方案；提出的SLO-ODBS算法通过综合考虑SLO约束和输出长度差异，实现了低延迟与低SLO违约率的双重优化目标；提出的HELR算法将LLM部署建模为拓扑感知动态规划问题，无需元学习压力测试即可自动求解最优设备映射。在4块NVIDIA RTX 3090 GPU构成的异构集群上，UELLM相比S$^3$和Morphling将推理延迟降低72.3\%至90.3\%，GPU利用率提升1.2倍至4.1倍，吞吐量提高1.92倍至4.98倍，并实现零SLO违约，验证了批处理调度与部署配置联合优化的有效性。

\item 建立了PD分离系统的多目标性能优化理论模型，定量刻画了计算-显存资源失衡的内在机理。针对PD分离架构中Prefill实例（计算利用率约95\%，显存利用率约35\%）与Decode实例（计算利用率约35\%，显存利用率约90\%）的天然资源互补性，建立了涵盖首token延迟（TTFT）、每输出token时间（TPOT）、跨实例资源利用率和模块迁移开销的多目标联合优化模型。通过对层级KV Cache传输时间（$T_{KV} \approx 0.082$ ms）与每层前向计算时间（$T_{F,\text{layer}} \approx 4.22$ ms）的定量分析，从理论上证明了三阶段层级流水线重叠传输的可行性，为全局KV Cache存储的设计提供了严格的理论依据。

\item 提出了面向PD分离架构的动态资源协同优化框架BanaServe，设计了粗细粒度自适应迁移机制。针对现有PD分离系统中静态资源配置无法适应动态负载、前缀缓存感知路由引发持续性负载倾斜的问题，提出了BanaServe框架，通过三项核心机制实现资源分配与状态管理的解耦：层级权重迁移在Transformer层粒度上同步迁移权重和KV Cache（迁移延迟$T_{\text{layer}} \approx S^{\text{total}}_\ell / B_{\text{net}} + T_{\text{sync}}$），实现粗粒度跨实例资源再平衡；注意力级KV Cache迁移沿注意力头维度分割KV Cache，仅传输KV状态（$T_{\text{attn}} \approx S_{kv}/B_{\text{net}} \ll T_{\text{layer}}$），实现细粒度负载均衡且无服务中断；全局KV Cache存储配合三阶段层级流水线重叠传输，使路由器摆脱缓存位置约束，实现纯负载感知调度。在合成基准（Alpaca和LongBench）评估中，相比vLLM吞吐量提升1.2倍至3.9倍，总处理时间降低3.9\%至78.4\%；相比DistServe吞吐量提升1.1倍至2.8倍，延迟降低1.4\%至70.1\%。

（4）在Azure生产负载追踪数据上验证了BanaServe在真实生产条件下的鲁棒性与泛化能力。采用Azure公开发布的LLM推理生产负载追踪数据（包含重尾输入/输出长度分布和高突发性到达模式），在LLaMA-13B和OPT-13B两个模型架构上进行了系统性验证。实验结果表明，BanaServe在LLaMA-13B上相比vLLM吞吐量提升40.4\%、延迟降低71.2\%，在OPT-13B上相比vLLM吞吐量提升58.3\%、延迟降低74.1\%，在两种模型架构上均实现最高吞吐量和最低延迟，表明所提方法对不同Transformer实现具有良好的泛化能力。

在研究展望方面，本文的工作尚存在若干值得进一步深入探索的方向。（1）异构硬件感知调度方面，现有迁移延迟模型对混合精度加速器和异构互联拓扑的建模较为简化，未来可构建细粒度设备画像数据库并设计拓扑感知的迁移路径规划算法，进一步释放异构硬件集群的潜在性能。（2）预测性主动编排方面，现有BanaServe的被动响应式调度在极端突发场景下存在不可避免的响应延迟窗口，可引入基于历史负载模式的时序预测模型和强化学习驱动的迁移策略优化，将被动响应转变为主动预分配。（3）跨地域分布式推理方面，现有设计局限于单数据中心集群，在WAN高延迟（10--100 ms）环境下三阶段流水线重叠传输的有效性条件（$T_{KV} \ll T_{F,\text{layer}}$）可能不再成立，需要设计广域网感知的层次化KV Cache同步架构和地理感知请求路由策略。（4）多模型协同推理方面，随着RAG、工具调用Agent等多模型协同范式的普及，如何在同一集群上高效调度异构模型实例并实现跨模型KV Cache共享，是值得深入探索的重要研究方向。

\end{enumerate}

\end{conclusion}

