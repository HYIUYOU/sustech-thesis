% !TeX root = ../sustechthesis-example.tex

% 第1章 绪论
\chapter{绪论}

\section{研究背景与意义}

\subsection{研究背景}

近年来，以GPT-4\cite{achiam2023gpt4}、LLaMA\cite{touvron2023llama}、Claude\cite{anthropic2025claude}等为代表的大语言模型（Large Language Models, LLMs）在自然语言处理、代码生成、知识检索和内容创作等领域展现出卓越的性能，推动了人工智能技术的跨越式发展。然而，随着模型参数规模从数十亿增长至数千亿甚至万亿级别，其训练和推理过程对计算资源提出了极高的要求。据统计，云平台中约90\%的人工智能计算资源被用于模型推理服务而非训练\cite{wu2022sustainable}，且推理成本随着模型规模呈指数级增长。例如，为维持ChatGPT日均7000万次访问的服务规模，需要部署超过6万张NVIDIA A100 GPU，初始投资成本高达16亿美元，每日电费支出约10万美元\cite{he2024uellm}。

在典型的机器学习即服务（MLaaS）架构中，开发者首先利用大规模数据集离线训练LLMs，随后将训练好的模型部署到分布式云环境中提供在线推理服务。由于单张GPU的显存容量有限（通常为40GB或80GB），而千亿级模型的参数和激活值往往超出单机显存容量，因此必须采用分布式部署策略。然而，随着部署GPU数量的增加，设备间的通信开销和同步延迟显著上升；反之，若GPU数量不足，则会导致显存溢出（Out-of-Memory, OOM）错误或极长的推理延迟。此外，大模型推理具有独特的两阶段执行特性：预填充（Prefill）阶段和解码（Decode）阶段。Prefill阶段计算密集，需并行处理整个输入序列以生成第一令牌（Time to First Token, TTFT）；Decode阶段则受限于自回归特性，逐令牌生成输出，呈现内存密集型特征，其关键指标为每输出令牌时间（Time Per Output Token, TPOT）。这种计算-内存需求的固有不对称性，使得传统单一架构难以同时优化两个阶段，导致严重的资源利用率低下和负载不均衡问题。

另一方面，生产环境的推理负载具有高度动态性和不可预测性。请求到达率（Requests Per Second, RPS）随时间剧烈波动，输入/输出序列长度分布呈现重尾特性（Heavy-tailed）。传统静态资源配置策略在负载低谷期造成严重的资源浪费（GPU利用率仅20\%-40\%），而在突发流量（Bursty Traffic）下又因扩容滞后导致服务等级目标（Service Level Objective, SLO）违约甚至服务中断。现有系统如vLLM\cite{kwon2023efficient}通过PagedAttention优化显存管理，DistServe\cite{zhong2024distserve}采用PD分离架构消除阶段间干扰，但这些方案仍受限于粗粒度的资源配置（实例级或GPU池级）和缓存感知的静态路由策略，无法适应快速变化的负载模式。因此，如何在保障严格SLO的前提下，实现计算与内存资源的细粒度弹性调度，成为大模型推理服务面临的核心挑战。

\subsection{研究意义}

本研究围绕大模型推理服务的批式调度与弹性伸缩展开，具有以下重要的理论价值和实践意义：

从科学价值角度，本研究针对LLM推理中计算-内存协同优化这一基础科学问题，揭示了静态资源配置与动态负载需求之间的结构性矛盾，提出了从请求级批处理到模块级弹性伸缩的分层优化理论框架。通过建立考虑KV Cache动态增长、网络拓扑异构性和SLO约束的数学模型，探索了在离散配置空间中寻找最优部署策略的算法边界，为大规模分布式推理系统的资源管理提供了新的理论依据。

从工程实践角度，研究成果可直接应用于云原生AI基础设施，具有显著的经济效益和社会价值。首先，通过精确的输出长度预测和SLO感知的批处理算法，可减少无效填充（Padding）和冗余计算，预计可降低推理成本30\%以上；其次，模块级细粒度迁移技术打破了传统副本级扩缩容的粒度限制，将资源响应时间从分钟级降至秒级，显著提升系统对突发流量的鲁棒性；再者，跨实例的KV Cache共享机制消除了缓存局部性对调度策略的约束，解决了前缀缓存感知路由导致的热点倾斜问题，提高了集群整体资源利用率。这些技术对推动大模型在智能客服、自动驾驶、金融风控等延迟敏感场景的普及应用具有重要意义，同时通过提升资源利用效率减少了能源消耗，符合绿色计算和可持续发展的战略目标。

此外，本研究提出的统一优化框架兼顾算法效率与系统可实现性，已在真实GPU集群和Azure生产环境traces中验证了其有效性，为学术界和工业界提供了可复现、可部署的技术路径，有助于填补当前LLM推理服务在细粒度资源管理方面的研究空白。

\section{国内外研究现状及分析}

本节从大模型推理优化的技术路径出发，系统梳理了批处理调度、资源配置优化和PD分离架构三个维度的研究现状，分析现有方案的局限性，并引出本文的研究切入点。

\subsection{批处理与请求调度优化}

批处理（Batching）是提升LLM推理吞吐量的关键技术。近期的vLLM\cite{kwon2023efficient}提出了PagedAttention机制，通过块级KV Cache管理减少显存碎片，支持动态批处理（Continuous Batching）。SGLang\cite{zheng2024sglang}通过前缀缓存（Prefix Caching）和缓存感知路由减少预填充阶段的冗余计算。然而，这些系统的批处理策略主要依据请求到达顺序（FIFO）或简单启发式规则，缺乏对请求输出长度的先验知识，导致批内请求长度差异过大时产生严重的填充浪费（Padding Waste）。

针对此问题，S$^3$\cite{jin2023s3}将批处理建模为装箱问题，通过预测输出长度优化请求组合，但仅考虑内存优化而未考虑SLO约束。BATCH\cite{ali2020batch}提出了针对无服务器平台的自适应批处理框架，但采用穷举搜索配置，时间复杂度高。Tabi\cite{wang2023tabi}针对判别式模型优化，难以应用于生成式LLMs。现有方案普遍存在三个缺陷：一是缺乏对请求SLO的显式建模，导致高优先级请求可能被延迟；二是未考虑KV Cache随生成过程动态增长的特性；三是静态批处理策略无法适应负载变化。

\subsection{模型部署与资源配置优化}

在资源配置方面，MArk\cite{zhang2019mark}提出了面向SLA的推理服务系统，集成IaaS和无服务器计算以降低成本，但主要针对小型传统模型。Morphling\cite{wang2021morphling}采用元学习（Meta-learning）快速搜索最优配置，但需要对每个候选配置进行压力测试，在LLM场景下引入显著延迟。Splitwise\cite{patel2024splitwise}和DistServe\cite{zhong2024distserve}探索了PD分离架构，将Prefill和Decode阶段分配到不同GPU实例以消除阶段间干扰，但采用静态的资源配比，无法适应动态负载。

现有PD分离方案存在固有资源失衡问题：Prefill实例通常计算利用率超过95\%但显存利用率仅35\%，而Decode实例则相反。Mooncake\cite{qin2024mooncake}和MemServe\cite{hu2024memserve}尝试通过分布式KV Cache池解决内存瓶颈，但引入跨节点查找开销和一致性协调复杂性。总体而言，当前系统缺乏细粒度的在线资源重平衡机制，难以在请求级动态调整资源配置。

\subsection{弹性伸缩与负载均衡}

弹性伸缩（Auto-scaling）是应对动态负载的关键技术。传统方法如Kubernetes的Horizontal Pod Autoscaler（HPA）采用副本级（Replica-level）扩缩容，粒度粗糙，启动延迟长达1-30分钟，无法应对LLM推理的突发流量。Llumnix\cite{sun2024llumnix}提出了动态负载均衡策略，但未针对PD分离架构中的计算-内存异构性进行优化。SpotServe\cite{miao2024spotserve}利用可抢占实例降低成本，但缺乏对LLM特定访存模式的优化。

在细粒度资源管理方面，Choi等\cite{choi2022serving}提出了GPU时空共享的gpulet抽象，但仅适用于小型传统模型。现有研究尚未解决PD分离架构中模块级（Layer-level）的动态迁移问题，无法在Prefill和Decode实例之间实时转移计算负载或显存压力，导致资源碎片化严重。

\subsection{现有研究的不足与分析}

综合上述分析，现有LLM推理系统在以下三个方面存在显著局限：

\textbf{（1）调度策略与资源状态紧耦合}：现有系统（如SGLang、Mooncake）的调度决策严重依赖KV Cache的物理位置，前缀缓存感知路由导致负载热点倾斜。路由器被迫在计算负载均衡与缓存命中率之间做困难权衡，无法基于实时负载做出最优调度。

\textbf{（2）资源配置静态化与粗粒度}：现有PD分离系统（DistServe、Splitwise）在部署时固定Prefill与Decode实例比例，无法在运行期间根据实际负载动态调整。副本级扩缩容响应滞后，难以处理突发流量，且模型加载开销巨大。

\textbf{（3）缺乏跨阶段的细粒度资源协同}：Prefill与Decode阶段的资源需求（计算vs内存）互补，但现有系统缺乏在两个阶段之间动态迁移工作负载或显存数据的机制，导致严重的资源利用率失衡。

针对上述问题，本文拟从请求级批处理优化和模块级弹性伸缩两个层面，研究大模型推理服务的统一资源管理框架。

\section{论文主要工作}

针对大模型推理服务中资源利用率低、SLO保障困难、动态适应性差等挑战，本文围绕批式调度与弹性伸缩两个核心问题，系统性地提出了资源画像、动态批处理、智能部署与细粒度迁移/复制的优化方法，主要贡献包括以下两个方面：

\subsection{统一高效的批式调度与部署框架UELLM}

针对传统推理系统批处理策略粗放、部署配置静态化、缺乏SLO感知等问题，本文提出了UELLM（Unified and Efficient LLM Inference Serving）框架，实现了请求级资源画像、动态批处理与高效部署的协同优化。

\textbf{（1）基于微调的推理任务资源画像方法}：针对LLM推理中输出长度不确定导致的资源需求难以预测问题，本文采用基于微调大模型（ChatGLM3-6B）的方法对请求输出长度进行预测。通过在Alpaca、Natural Questions等指令遵循数据集上微调，模型对长度分桶的预测准确率达到99.51\%，为后续调度提供先验知识。

\textbf{（2）SLO与输出长度驱动的动态批处理算法（SLO-ODBS）}：突破了传统FIFO批处理的局限，建立了综合考虑请求SLO约束和输出长度差异的批处理优化模型。算法通过权重化目标函数平衡总延迟与总输出长度，采用贪心策略将请求组合成批，显著减少KV Cache冗余和填充浪费。实验表明，SLO-ODBS在保持低延迟的同时，将SLO违约率降低至接近0\%。

\textbf{（3）高效低延迟资源分配算法（HELR）}：针对LLM分布式部署中设备映射（Device Map）搜索空间巨大、静态配置性能次优的问题，提出了基于动态规划的高效资源分配算法。算法综合考虑集群网络拓扑（NVLink、PCIe带宽异构）、GPU计算能力差异和模型层间依赖关系，自动求解最优的层到设备映射策略。通过调整权重系数，可同时优化GPU利用率（HE模式）或推理延迟（LR模式）。

UELLM框架在真实4-GPU集群上的验证表明，相比Morphling和S$^3$等先进方案，系统降低推理延迟72.3\%至90.3\%，提升GPU利用率1.2倍至4.1倍，吞吐量提高1.92倍至4.98倍，且实现了零SLO违约的推理服务。

\subsection{面向PD分离架构的细粒度弹性伸缩框架BanaServe}

针对PD（Prefill-Decode）分离架构中固有的计算-内存负载失衡、静态资源配置适应性差、前缀缓存导致路由倾斜等问题，本文提出了BanaServe动态编排框架，实现了模块级细粒度迁移与全局KV Cache共享。

\textbf{（1）模块级细粒度动态迁移机制}：突破传统副本级扩缩容的粒度限制，提出了层级（Layer-level）权重迁移与注意力级（Attention-level）KV Cache迁移两种机制。层级迁移支持将连续的Transformer层动态迁移至负载较轻的GPU，实现粗粒度负载重平衡；注意力级迁移将KV Cache按注意力头维度切分，将部分头的计算卸载至辅助GPU（Cold GPU），主GPU（Hot GPU）与辅助GPU并行计算，在不迁移模型权重的情况下实现细粒度负载分担。数学上证明了注意力分解的正确性，确保分布式计算与单卡计算数值等价。

\textbf{（2）全局KV Cache存储与层间流水线传输}：为消除前缀缓存对路由决策的约束，设计了跨Prefill实例的全局KV Cache存储层，结合CPU/SSD作为持久化后端。针对全局存储引入的访问延迟，提出了层间流水线重叠传输机制，利用Transformer逐层计算特性，将第$i$层计算与第$i+1$层KV Cache预取重叠，隐藏通信开销。理论分析与实验验证表明，当KV Cache传输时间（约0.082ms）远小于层计算时间（约4.22ms）时，可实现近透明的缓存访问。

\textbf{（3）负载感知请求调度算法}：基于全局KV Cache存储，实现了完全基于实时负载的调度策略，无需考虑缓存局部性。算法周期性地测量各Prefill实例的综合负载（计算+内存），将新请求分发至负载最轻的实例，并配合动态迁移机制实现快速负载均衡。

BanaServe在13B参数模型（LLaMA-13B、OPT-13B）和公开基准（Alpaca、LongBench、Azure生产环境traces）上的评估表明，相比vLLM，系统吞吐量提升1.2倍至3.9倍，延迟降低3.9\%至78.4\%；相比DistServe，吞吐量提升1.1倍至2.8倍，延迟降低1.4\%至70.1\%，并在突发流量下展现出卓越的鲁棒性。

\subsection{主要创新点}

本文的主要创新点可总结为：

\textbf{（1）提出了SLO感知的动态批处理理论框架}：首次将输出长度预测与SLO约束联合建模，突破了传统批处理仅优化吞吐量的局限，实现了延迟违约率与资源利用率的联合优化。

\textbf{（2）提出了基于网络拓扑感知的LLM部署优化方法}：将模型层分配问题形式化为带约束的动态规划问题，综合考虑异构互联拓扑和计算能力差异，解决了传统方法部署配置次优或搜索开销过高的问题。

\textbf{（3）提出了模块级细粒度弹性伸缩新范式}：将扩缩容粒度从实例级下沉至Transformer层/注意力头级，实现了PD分离架构中计算与内存资源的在线重平衡，填补了细粒度在线资源迁移研究空白。

\textbf{（4）提出了全局KV Cache存储与计算-通信重叠机制}：通过解耦缓存状态与计算位置，消除了前缀缓存对调度的约束，结合流水线传输解决了全局存储的延迟瓶颈。

\section{论文组织架构}

本文共分为六章，各章内容安排如下：

\textbf{第一章 绪论}：介绍大模型推理服务的研究背景与意义，分析批式调度和弹性伸缩领域的国内外研究现状，阐述本文的主要工作与创新点。

\textbf{第二章 相关技术与理论基础}：介绍大语言模型Transformer架构、KV Cache机制、PD分离架构等背景知识；阐述资源管理中的关键metrics（TTFT、TPOT、SLO等）；分析现有系统架构（vLLM、DistServe等）的技术细节与局限性。

\textbf{第三章 UELLM：统一高效的批式调度与部署}：详细介绍UELLM系统架构，包括资源画像模块的设计与实现、SLO-ODBS批处理算法的数学建模与算法流程、HELR部署优化算法的动态规划求解过程，以及系统集成与实现细节。

\textbf{第四章 BanaServe：面向PD分离架构的细粒度弹性伸缩}：阐述BanaServe的整体架构设计，详细论述层级迁移与注意力级迁移的数学原理与实现机制，介绍全局KV Cache存储的设计与层间流水线传输优化，以及负载感知调度算法与动态决策流程。

\textbf{第五章 实验验证与性能评估}：介绍实验环境搭建（硬件配置、测试模型、数据集），设计对比实验验证UELLM在批处理、部署优化方面的性能提升，验证BanaServe在不同负载模式、上下文长度、生产环境traces下的吞吐量和延迟表现，并进行消融实验分析各组件的贡献。

\textbf{第六章 总结与展望}：总结本文的主要研究成果，讨论存在的问题与局限性，展望未来研究方向，包括异构硬件感知调度、预测性弹性伸缩、跨地域分布式推理等。