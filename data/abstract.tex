% !TeX root = ../sustechthesis-example.tex

% 中英文摘要和关键字

\begin{abstract}
  随着大语言模型（LLMs）在云计算环境中的广泛部署，其推理服务面临资源消耗巨大、服务等级目标（SLO）要求严格、预填充（Prefill）与解码（Decode）阶段资源需求异构以及负载动态多变等多重挑战。据统计，云平台中约90\%的人工智能计算资源用于模型推理而非训练，而现有系统受限于粗粒度的静态资源配置，GPU利用率通常仅为20\%--40\%，且在突发流量下因扩容滞后导致严重的SLO违约。针对这些问题，本文围绕"面向大模型的推理任务批式调度和KV缓存优化"这一主题，构建了从资源画像、静态优化到动态调控的递进式技术体系，实现了对LLM推理服务全生命周期资源管理的系统性优化。

  首先，针对LLM推理任务资源需求高度不确定、传统基于峰值预留策略导致严重资源浪费的问题，本文提出了面向异构LLM任务的细粒度资源画像与预测方法。通过分析任务类型、输入长度、语言类型等多维属性，设计并采集了182个覆盖不同复杂度的测试样本；针对混合数据类型特征，提出基于K-Prototypes算法的负载聚类方法，通过定义混合距离函数（欧氏距离+简单匹配距离）将任务划分为6个具有显著差异的资源消费模式；在每个聚类内部，采用Random Forest、Gradient Boosting和LightGBM等集成学习方法，对GPU利用率、显存占用和推理时延等五维资源指标进行预测。该方法在80\%测试样本上相对误差小于10\%，为后续资源调度提供了可靠的先验知识。

  其次，基于资源画像获取的任务先验知识，针对静态场景下批处理策略粗放、部署配置次优等问题，本文提出了UELLM（Unified and Efficient LLM Inference Serving）统一高效批式调度与部署框架。该框架通过微调ChatGLM3-6B模型对请求输出长度进行预测，分桶准确率达到99.51\%；设计了SLO与输出长度驱动的动态批处理算法（SLO-ODBS），通过双阶段贪心策略优化请求组合，减少KV Cache冗余填充；提出了基于动态规划的高效低延迟资源分配算法（HELR），综合考虑集群网络拓扑异构性（NVLink/PCIe带宽差异）自动优化层到GPU的映射，支持高利用率（HE）与低延迟（LR）两种模式切换。在4-GPU集群上的验证表明，UELLM相比Morphling和S$^3$等先进方案，降低推理延迟72.3\%至90.3\%，提升GPU利用率1.2倍至4.1倍，吞吐量提高1.92倍至4.98倍，并实现零SLO违约。

  再次，面向PD（Prefill-Decode）分离架构中固有的计算--内存负载失衡、静态资源配置适应性差、前缀缓存感知路由导致的热点倾斜等问题，本文提出了BanaServe细粒度弹性伸缩框架。该框架创新性地引入了模块级细粒度迁移机制，包括层级（Layer-level）权重迁移与注意力级（Attention-level）KV Cache迁移，前者支持连续Transformer层的动态重配置以实现粗粒度负载均衡，后者通过按注意力头切分KV Cache实现细粒度计算卸载，数学上证明了注意力分解的数值等价性；设计了全局KV Cache存储与层间流水线重叠传输机制，通过解耦缓存状态与计算位置消除前缀缓存对调度的约束，并利用层间流水线隐藏通信延迟；提出了基于实时负载感知的请求调度算法，配合动态迁移机制实现快速负载均衡。在13B参数模型（LLaMA-13B/OPT-13B）和公开基准（Alpaca/LongBench）上的评估表明，BanaServe相比vLLM吞吐量提升1.2倍至3.9倍、延迟降低3.9\%至78.4\%；相比DistServe吞吐量提升1.1倍至2.8倍、延迟降低1.4\%至70.1\%，并在Azure生产环境traces下展现出优异的鲁棒性。

  本文所提出的方法在真实GPU集群和公开基准数据集上得到了充分验证，构建的"资源画像--静态优化--动态调控"闭环技术体系，为大模型推理服务的高效部署与资源管理提供了新的理论依据和技术路径。

  \thusetup{
    keywords = {大语言模型推理, 批式调度, 弹性伸缩, 资源画像, K-Prototypes聚类, 集成学习, PD分离架构, 动态迁移, KV Cache优化},
  }
\end{abstract}

\begin{abstract*}
  With the widespread deployment of Large Language Models (LLMs) in cloud computing environments, inference services face critical challenges including massive resource consumption, strict Service Level Objective (SLO) requirements, heterogeneous resource demands between the prefill and decode phases, and highly dynamic workload variations. Statistics indicate that approximately 90\% of AI computing resources in cloud platforms are dedicated to model inference rather than training. However, existing systems are constrained by coarse-grained static resource configurations, resulting in GPU utilization typically ranging merely between 20\%--40\%, and severe SLO violations due to delayed scaling during bursty traffic. To address these issues, this thesis focuses on ``Batch Scheduling and KV Cache Optimization for Large Language Model Inference,'' constructing a progressive technical architecture spanning from resource profiling and static optimization to dynamic scaling, thereby achieving systematic optimization of full-lifecycle resource management for LLM inference services.

  Firstly, to tackle the highly uncertain resource demands of LLM inference tasks and the severe resource waste caused by traditional peak-based reservation strategies, this thesis proposes a fine-grained resource profiling and prediction method for heterogeneous LLM tasks. By analyzing multi-dimensional attributes including task types, input lengths, and language modalities, we design and collect 182 test samples covering various complexity scenarios. Addressing the mixed data type characteristics, we introduce a K-Prototypes-based load clustering method that partitions tasks into six distinct resource consumption patterns through a hybrid distance function (Euclidean distance for numerical features and simple matching distance for categorical features). Within each cluster, ensemble learning methods including Random Forest, Gradient Boosting, and LightGBM are employed to predict five-dimensional resource metrics comprising GPU utilization, VRAM occupancy, and inference latency. This method achieves a relative error of less than 10\% on 80\% of test samples, providing reliable prior knowledge for subsequent resource scheduling.

  Secondly, leveraging the task prior knowledge obtained from resource profiling, this thesis proposes UELLM (Unified and Efficient LLM Inference Serving), a comprehensive batch scheduling and deployment optimization framework addressing coarse-grained batching strategies and suboptimal deployment configurations in static scenarios. The framework employs a fine-tuned ChatGLM3-6B model to predict request output lengths, achieving a bucketing accuracy of 99.51\%. It introduces the SLO and Output-Driven Dynamic Batch Scheduler (SLO-ODBS), which optimizes request combinations through a two-stage greedy strategy to reduce KV Cache redundancy and padding overhead. Additionally, the High-Efficiency Low-Latency Resource Allocation (HELR) algorithm based on dynamic programming is proposed, which automatically optimizes layer-to-GPU mapping strategies considering cluster network topology heterogeneity (NVLink/PCIe bandwidth variations), supporting both High Efficiency (HE) and Low Latency (LR) modes. Experimental validation on a 4-GPU cluster demonstrates that UELLM reduces inference latency by 72.3\% to 90.3\%, improves GPU utilization by 1.2$\times$ to 4.1$\times$, and increases throughput by 1.92$\times$ to 4.98$\times$ compared to state-of-the-art methods such as Morphling and S$^3$, while achieving zero SLO violations.

  Thirdly, targeting the inherent compute-memory load imbalance, poor adaptability of static resource configurations, and hotspot skews caused by prefix cache-aware routing in PD (Prefill-Decode) disaggregated architectures, this thesis presents BanaServe, a fine-grained elastic scaling framework. The framework introduces novel module-level migration mechanisms, including layer-level weight migration for coarse-grained load rebalancing through dynamic reconfiguration of consecutive Transformer layers, and attention-level KV Cache migration for fine-grained computation offloading via head-wise partitioning of KV Cache, with mathematical proof of numerical equivalence for attention decomposition. It designs a Global KV Cache Store with layer-wise pipelined transmission to eliminate constraints imposed by prefix caching on scheduling decisions by decoupling cache state from compute placement, while hiding communication latency through inter-layer pipelining. Furthermore, a real-time load-aware request scheduling algorithm is proposed, working in coordination with dynamic migration for rapid load balancing. Evaluation on 13B-parameter models (LLaMA-13B and OPT-13B) and public benchmarks (Alpaca and LongBench) demonstrates that BanaServe achieves 1.2$\times$--3.9$\times$ higher throughput with 3.9\%--78.4\% lower latency compared to vLLM, and 1.1$\times$--2.8$\times$ throughput improvement with 1.4\%--70.1\% latency reduction compared to DistServe, exhibiting superior robustness under Azure production traces.

  The proposed methods have been extensively validated on real GPU clusters and public benchmarks. The closed-loop technical architecture of ``Resource Profiling -- Static Optimization -- Dynamic Scaling'' provides novel theoretical foundations and technical pathways for efficient deployment and resource management of LLM inference services.

  \thusetup{
    keywords* = {Large Language Model Inference, Batch Scheduling, Elastic Scaling, Resource Profiling, K-Prototypes Clustering, Ensemble Learning, PD Disaggregated Architecture, Dynamic Migration, KV Cache Optimization},
  }
\end{abstract*}