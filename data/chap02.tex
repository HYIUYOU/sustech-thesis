% !TeX root = ../sustechthesis-example.tex

% 现有LLM的推理任务批式任务调度和KV 缓存优化的方法

% 1. 现有的LLM的推理任务的批式任务调度方法
%    1. S3
%    2. continues batching
%    3. Orca 中的In-flight Batching批处理机制
%    4. 传统组合（FIFO，短作业优先，长作业优先）
% 2. 现有的KV 缓存优化方法
%    1. mooncake
%    2. memserve
%    3. vLLM
%    4. 量化
% 3. 现有方法的缺陷

\chapter{现有LLM的推理任务批式任务调度和KV缓存优化的方法}
\label{chap:related-work}

大语言模型推理优化是一个快速发展的研究领域，学术界和工业界提出了众多技术方案。本章系统梳理现有LLM推理任务的批式调度方法与KV缓存优化技术，分析各类方案的设计思想、核心机制与适用场景，并总结现有方法的局限性，为后续章节提出UELLM和BanaServe两种优化方案提供技术背景与改进动机。

\section{现有的LLM推理任务批式调度方法}
\label{sec:batch-scheduling}

批处理（Batching）是提升LLM推理吞吐量的核心手段。通过将多个请求合并处理，可以充分利用GPU的并行计算能力，提高硬件利用率。本节介绍从传统调度策略到面向LLM特性的先进批处理技术的演进过程。

\subsection{S$^3$：输出长度感知调度}
\label{subsec:s3}

S$^3$（SLO-aware, Size-based, Scheduling System）\cite{jin2023s3}是针对生成式LLM推理服务提出的批处理优化框架，其核心洞察是：\textbf{输出序列长度的差异}是导致批处理效率低下的关键因素。

传统批处理策略将不同输出长度的请求强制填充（Padding）到同一批次，导致大量无效计算。如图~\ref{fig:s3-illustration}所示，当短输出请求（如8 tokens）与长输出请求（如128 tokens）被批处理时，短请求必须等待长请求完成，造成严重的计算资源浪费。

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        box/.style={draw, minimum width=2.5cm, minimum height=0.8cm, align=center, font=\small},
        short/.style={fill=green!20},
        long/.style={fill=red!20},
        arrow/.style={->, thick}
    ]
        % 传统批处理
        \node[box, short] (req1) at (0,2) {请求1\\8 tokens};
        \node[box, long] (req2) at (3,2) {请求2\\128 tokens};
        \node[box, long, minimum width=6cm] (batch1) at (1.5,0.5) {批处理结果：填充至128 tokens};
        
        \draw[arrow] (req1) -- (batch1);
        \draw[arrow] (req2) -- (batch1);
        
        \node at (1.5, -0.8) {\textbf{传统方法：}大量填充，资源浪费};
        
        % S3批处理
        \node[box, short] (req3) at (8,2) {请求1\\8 tokens};
        \node[box, long] (req4) at (11,2) {请求2\\128 tokens};
        \node[box, short] (batch2) at (8,0.5) {批次A：8 tokens};
        \node[box, long] (batch3) at (11,0.5) {批次B：128 tokens};
        
        \draw[arrow] (req3) -- (batch2);
        \draw[arrow] (req4) -- (batch3);
        
        \node at (9.5, -0.8) {\textbf{S3方法：}按长度分组，减少填充};
    \end{tikzpicture}
    \caption{S3与传统批处理策略对比}
    \label{fig:s3-illustration}
\end{figure}

S3的核心创新包括：

\textbf{(1) 输出长度预测模型}。S3采用轻量级预测器（基于输入文本特征）估计每个请求的输出行长度，将长度相近的请求归入同一批次。预测模型在多种工作负载上达到85\%以上的准确率。

\textbf{(2) 多维装箱调度算法}。将请求调度建模为多维装箱问题，综合考虑输出长度、SLO约束和GPU显存容量，采用启发式算法优化批次组合。目标函数为：
\begin{equation}
    \min \sum_{i=1}^{N} \left( \text{Padding}_i + \lambda \cdot \text{SLO\_Violation}_i \right)
\end{equation}
其中$\text{Padding}_i$为第$i$个批次的填充开销，$\text{SLO\_Violation}_i$为SLO违约惩罚，$\lambda$为权衡系数。

\textbf{(3) 动态批次重构}。在请求到达和完成时动态调整批次组成，最大化吞吐量同时保证延迟约束。

S3的局限性在于：仅优化批处理组合，未考虑模型部署配置对性能的影响；缺乏对网络拓扑异构性的感知；预测模型针对特定数据集训练，泛化能力有限。

\subsection{Continuous Batching：动态请求合并}
\label{subsec:continuous-batching}

Continuous Batching（连续批处理）\cite{kwon2023efficient}是vLLM提出的动态批处理机制，打破了传统静态批处理"等待-处理-等待"的串行模式。

\textbf{核心机制}：在Decode阶段，每当有请求完成生成并释放资源时，系统立即从等待队列中选取新请求加入当前批次，保持GPU始终处于满负荷状态。如图~\ref{fig:continuous-batching}所示，与传统静态批处理相比，Continuous Batching显著减少了GPU空闲时间。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{example-image-a}
    \caption{Static Batching与Continuous Batching的时间线对比}
    \label{fig:continuous-batching}
\end{figure}

Continuous Batching的技术要点：

\textbf{(1) 迭代级调度}。以迭代（iteration）而非请求为粒度进行调度，每个Decode步骤完成后重新评估批次组成。

\textbf{(2) 显存动态管理}。配合PagedAttention技术（见第~\ref{subsec:vllm}节），动态分配和释放KV Cache显存，支持变长序列的高效批处理。

\textbf{(3) 抢占与恢复}。当高优先级请求到达时，可抢占低优先级请求的显存资源，被抢占请求稍后恢复执行。

Continuous Batching的优势在于高吞吐量和低延迟，但其调度决策仅基于当前系统状态，缺乏对未来负载的预测能力；同时，未考虑Prefill与Decode阶段的资源需求差异，在混合负载场景下可能出现资源竞争。

\subsection{Orca中的In-flight Batching批处理机制}
\label{subsec:orca}

Orca\cite{yu2022orca}是NVIDIA提出的LLM推理服务系统，其\textbf{In-flight Batching}（飞行中批处理）机制进一步优化了Continuous Batching，实现了Prefill与Decode阶段的流水线并行。

\textbf{关键观察}：在LLM推理中，Prefill阶段（计算密集）和Decode阶段（内存密集）对GPU资源的需求互补。传统系统串行执行两个阶段，导致资源利用率低下。

In-flight Batching的核心设计：

\textbf{(1) 阶段内批处理}。将Prefill和Decode分别批处理，但允许两个阶段在同一GPU上交替执行。当一批请求的Prefill完成后，立即开始Decode，同时GPU Tensor Core可处理下一批Prefill请求。

\textbf{(2) 细粒度同步}。采用迭代级同步而非请求级同步，减少流水线气泡（bubble）。如图~\ref{fig:inflight-batching}所示，通过精细的流水线调度，GPU计算单元和内存带宽得到更充分利用。

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        timeslot/.style={draw, minimum width=1.2cm, minimum height=0.6cm, font=\tiny},
        prefill/.style={fill=red!30},
        decode/.style={fill=blue!30},
        label/.style={font=\small\bfseries}
    ]
        % 时间轴
        \foreach \x in {0,1,2,3,4,5,6,7,8} {
            \draw (\x,0) -- (\x,-0.1);
            \node[font=\tiny] at (\x,-0.3) {\x};
        }
        
        % 传统方法
        \node[label] at (-1,1.5) {传统};
        \node[timeslot, prefill] at (0.5,1.5) {P};
        \node[timeslot, prefill] at (1.5,1.5) {P};
        \node[timeslot, decode] at (2.5,1.5) {D};
        \node[timeslot, decode] at (3.5,1.5) {D};
        \node[timeslot, prefill] at (4.5,1.5) {P};
        \node[timeslot, prefill] at (5.5,1.5) {P};
        \node[timeslot, decode] at (6.5,1.5) {D};
        \node[timeslot, decode] at (7.5,1.5) {D};
        
        % In-flight Batching
        \node[label] at (-1,0.5) {Orca};
        \node[timeslot, prefill] at (0.5,0.5) {P1};
        \node[timeslot, prefill] at (1.5,0.5) {P2};
        \node[timeslot, decode, fill=blue!20] at (2,0.5) {D1};
        \node[timeslot, prefill] at (2.5,0.5) {P3};
        \node[timeslot, decode, fill=blue!40] at (3,0.5) {D1};
        \node[timeslot, decode, fill=blue!20] at (3.5,0.5) {D2};
        \node[timeslot, prefill] at (4,0.5) {P4};
        \node[timeslot, decode, fill=blue!40] at (4.5,0.5) {D2};
        \node[timeslot, decode, fill=blue!20] at (5,0.5) {D3};
        
        \draw[thick,->] (0,0) -- (8.5,0) node[right, font=\small] {时间};
    \end{tikzpicture}
    \caption{In-flight Batching流水线调度示意（P=Prefill, D=Decode）}
    \label{fig:inflight-batching}
\end{figure}

\textbf{(3) 选择性批处理}。对于Prefill阶段，仅对长度相近的请求批处理；对于Decode阶段，所有请求统一批处理，因为此时计算量与序列长度无关。

In-flight Batching的局限在于：虽然实现了阶段内并行，但未完全解耦Prefill和Decode的资源分配；缺乏对KV Cache全局共享的支持，在多实例场景下缓存命中率受限。

\subsection{传统调度策略}
\label{subsec:traditional-scheduling}

在LLM推理服务中，传统调度策略仍被广泛采用作为基准方法：

\textbf{(1) FIFO（First-In-First-Out，先进先出）}。按请求到达顺序处理，实现简单、公平性好。但FIFO对请求特性不敏感，短请求可能被长请求阻塞，导致平均等待时间较长。

\textbf{(2) SJF（Shortest Job First，短作业优先）}。优先处理预计执行时间短的请求。在LLM推理中，"短作业"对应输入长度短或输出长度短的请求。SJF可降低平均延迟，但可能导致长请求饥饿（starvation）。

\textbf{(3) LJF（Longest Job First，长作业优先）}。优先处理长请求，适用于需要尽快完成大任务的场景，但会显著增加短请求的等待时间。

\textbf{(4) 优先级调度}。根据用户等级、请求类型等分配优先级，高优先级请求优先处理。需配合抢占机制避免低优先级请求长期等待。

传统策略的共性局限在于：\textbf{静态决策}，不考虑系统当前负载和资源状态；\textbf{缺乏预测}，无法利用输出长度等先验信息优化调度；\textbf{粗粒度}，以请求为调度单位，未探索更细粒度的资源分配。

\section{现有的KV缓存优化方法}
\label{sec:kv-cache-optimization}

KV Cache（键值缓存）是LLM推理中的核心数据结构，用于存储注意力机制中的Key和Value张量，避免重复计算。然而，KV Cache的显存占用随批次大小和序列长度线性增长，成为推理扩展的主要瓶颈。本节介绍四种代表性的KV缓存优化技术。

\subsection{Mooncake：全局KV缓存池}
\label{subsec:mooncake}

Mooncake\cite{qin2024mooncake}是月之暗面（Moonshot AI）提出的KV Cache-centric推理架构，其核心思想是将KV Cache从GPU显存解耦，构建全局共享的缓存池。

\textbf{系统架构}：Mooncake将推理集群划分为Prefill集群和Decode集群，两者之间通过高速RDMA网络连接。Prefill集群计算并产生KV Cache，Decode集群消费KV Cache进行自回归生成。关键创新在于引入\textbf{分布式KV Cache池}，作为独立存储层管理所有缓存数据。

\textbf{核心技术}：

\textbf{(1) 分层缓存存储}。KV Cache池采用分层设计：热数据驻留GPU显存，温数据存放CPU内存，冷数据持久化到SSD。通过LRU（Least Recently Used）策略动态迁移数据，平衡访问延迟和存储成本。

\textbf{(2) 前缀复用}。利用请求间的前缀共享（如系统提示、多轮对话历史），通过前缀树（Trie）索引快速匹配缓存。匹配成功后，仅需计算增量部分的KV Cache，显著减少Prefill计算量。

\textbf{(3) 异步预取}。基于请求特征预测未来访问模式，异步将所需KV Cache从远程节点或下层存储预取到本地GPU，隐藏传输延迟。

Mooncake的优势在于高缓存命中率和弹性扩展能力，但其调度决策与缓存位置紧耦合：路由器必须考虑KV Cache的物理位置进行请求分发，导致负载不均衡——热点缓存节点可能过载，而冷节点资源闲置。

\subsection{MemServe：弹性内存管理}
\label{subsec:memserve}

MemServe\cite{hu2024memserve}聚焦于PD（Prefill-Decode）分离架构下的内存管理优化，提出\textbf{弹性内存池}（Elastic Memory Pool）概念。

\textbf{核心机制}：

\textbf{(1) 内存解耦}。将KV Cache存储与计算实例分离，构建独立的内存服务器集群。计算实例（GPU）通过高速网络访问远程内存，按需获取KV Cache。

\textbf{(2) 动态扩缩容}。根据负载变化动态调整内存池容量：高峰时段增加内存节点，低谷时段释放资源。结合Kubernetes等容器编排平台实现自动化运维。

\textbf{(3) 冗余消除}。识别并合并重复的KV Cache块（如共享前缀），采用引用计数管理生命周期，减少存储冗余。

MemServe的局限性在于：远程内存访问引入额外延迟，对网络带宽要求高；内存池的管理开销随规模增长，超大规模部署时可能成为瓶颈；未充分考虑Prefill与Decode实例间的负载均衡。

\subsection{vLLM：PagedAttention显存优化}
\label{subsec:vllm}

vLLM\cite{kwon2023efficient}是伯克利大学提出的开源LLM推理引擎，其\textbf{PagedAttention}技术从根本上解决了KV Cache的显存碎片问题。

\textbf{问题背景}：传统LLM推理系统将KV Cache存储为连续的显存块，如图~\ref{fig:memory-fragmentation}(a)所示。由于请求长度动态变化，频繁分配和释放导致严重的显存碎片，实际可用显存远低于物理容量。

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \begin{tikzpicture}[
            block/.style={draw, minimum width=0.8cm, minimum height=0.6cm, font=\tiny}
        ]
            \foreach \x/\c in {0/white, 1/red!30, 2/red!30, 3/white, 4/white, 5/blue!30, 6/white, 7/green!30, 8/green!30, 9/white} {
                \node[block, fill=\c] at (\x*0.9, 0) {};
            }
            \node[font=\small] at (4, -1) {碎片：3块，无法合并};
        \end{tikzpicture}
        \caption{传统连续存储}
        \label{subfig:contiguous}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \begin{tikzpicture}[
            block/.style={draw, minimum width=0.8cm, minimum height=0.6cm, font=\tiny}
        ]
            \foreach \x/\c in {0/red!30, 1/blue!30, 2/green!30, 3/red!30, 4/green!30, 5/white, 6/white, 7/white, 8/white, 9/white} {
                \node[block, fill=\c] at (\x*0.9, 0) {};
            }
            \draw[->, thick] (5*0.9, 0.5) -- (6*0.9, 0.5);
            \node[font=\small] at (4, -1) {非连续存储，无碎片};
        \end{tikzpicture}
        \caption{PagedAttention分页存储}
        \label{subfig:paged}
    \end{subfigure}
    \caption{显存碎片问题对比}
    \label{fig:memory-fragmentation}
\end{figure}

\textbf{PagedAttention核心设计}：

\textbf{(1) 块表映射}。将KV Cache划分为固定大小的块（Block，通常为512 tokens），通过块表（Block Table）记录逻辑块到物理块的映射关系。逻辑上连续的KV Cache在物理上可以分散存储。

\textbf{(2) 动态分配}。按需分配物理块，请求完成时立即回收。由于块大小固定，回收的块可立即被其他请求复用，消除外部碎片。

\textbf{(3) 共享机制}。通过块表引用计数实现KV Cache共享：多个请求共享同一前缀时，物理块只存储一份，块表记录多个引用。Copy-on-Write机制确保写操作的安全性。

PagedAttention使vLLM的显存利用率从传统系统的40-50\%提升至90\%以上，支持更大的批次和更长的上下文。但其设计针对单实例优化，在多实例分布式场景下缺乏全局缓存共享能力。

\subsection{量化：模型压缩与加速}
\label{subsec:quantization}

量化（Quantization）通过降低模型权重和激活值的数值精度，减少显存占用和计算量，是LLM推理优化的基础技术。

\textbf{(1) 权重量化（Weight Quantization）}。将FP16/BF16精度的模型权重压缩至INT8、INT4甚至更低比特。代表性方法包括：
\begin{itemize}
    \item \textbf{LLM.int8()} \cite{dettmers2022llm}：对异常值（outliers）保持FP16精度，其余权重用INT8表示，减少精度损失。
    \item \textbf{GPTQ} \cite{frantar2022gptq}：基于近似二阶信息的逐层量化，支持INT4精度且精度损失可控。
    \item \textbf{AWQ} \cite{lin2023awq}：激活感知的权重量化，保护对激活值敏感的重要权重。
\end{itemize}

\textbf{(2) KV Cache量化}。对KV Cache进行低比特量化，显著降低长上下文场景的显存压力。例如，将KV Cache从FP16量化为INT8，显存占用减半，支持2倍长的上下文。

\textbf{(3) 混合精度量化}。根据层的重要性或激活分布动态选择精度，关键层用高精度，次要层用低精度，平衡效率和精度。

量化的优势在于通用性强，可与前述批处理、缓存优化技术正交叠加。但其局限在于：量化本身不解决资源调度和负载均衡问题；极低比特量化（如INT4以下）可能显著影响模型能力；需要硬件支持低比特运算才能发挥加速效果。

\section{现有方法的缺陷}
\label{sec:limitations}

综合上述分析，当前LLM推理优化研究在以下维度存在显著局限，这些局限性构成了本文提出UELLM和BanaServe的改进动机。

\subsection{调度粒度与资源状态的紧耦合}

现有系统（如SGLang、Mooncake）的调度决策严重依赖KV Cache的物理位置。前缀缓存感知路由（Cache-aware Routing）虽然提高了缓存命中率，但引入了\textbf{负载热点倾斜}问题：路由器被迫在计算负载均衡与缓存命中率之间做困难权衡（Cache-Load Balancing Trade-off）。

具体表现为：高缓存命中率的节点持续接收新请求，计算负载迅速饱和；而低命中率节点虽有剩余算力，却因缺乏缓存数据无法分担负载。这种\textbf{正反馈效应}导致集群资源利用率严重失衡，部分节点过载而其他节点空闲。

\subsection{资源配置的静态化与刚性约束}

现有PD分离系统（DistServe、Splitwise、MemServe）在部署时固定Prefill与Decode实例比例，无法在运行期间根据实际负载动态调整。静态配置的问题在于：

\textbf{(1) 负载失配}。实际生产环境中，Prefill和Decode的负载比例随时间动态变化（如白天交互式应用Decode密集，夜间批处理任务Prefill密集）。固定比例导致某一阶段资源过剩而另一阶段成为瓶颈。

\textbf{(2) 扩容滞后}。突发流量（Bursty Traffic）下，静态系统无法快速重新分配资源，导致队列堆积和SLO违约。

\textbf{(3) 配置搜索开销}。Morphling等配置优化方法需要对每个候选配置进行压力测试，产生巨大时间和计算开销，难以适应快速变化的负载模式。

\subsection{缺乏跨阶段的细粒度资源协同机制}

Prefill与Decode阶段的资源需求（计算vs内存）在时域和空域上互补，但现有系统缺乏在两个阶段之间实时优化资源配置的\textbf{细粒度机制}。

如图~\ref{fig:resource-imbalance}所示，Prefill实例通常计算利用率>95\%但显存利用率<40\%，Decode实例则相反（计算<40\%，显存>90\%）。这种\textbf{资源利用率失衡}导致严重的资源浪费：Prefill实例的大量显存闲置无法被Decode利用，而Decode实例的计算能力未被充分利用。

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        box/.style={draw, minimum width=3cm, minimum height=2cm, align=center},
        bar/.style={draw, minimum width=0.6cm}
    ]
        % Prefill实例
        \node[box] (prefill) at (0,0) {\textbf{Prefill实例}\\[0.3cm]
            \begin{tikzpicture}[baseline]
                \node[bar, fill=red!70, minimum height=1.2cm] at (0,0) {};
                \node[bar, fill=blue!30, minimum height=0.4cm] at (0.8,0) {};
                \node[font=\tiny] at (0,-0.8) {计算95\%};
                \node[font=\tiny] at (0.8,-0.6) {显存35\%};
            \end{tikzpicture}
        };
        
        % Decode实例
        \node[box] (decode) at (5,0) {\textbf{Decode实例}\\[0.3cm]
            \begin{tikzpicture}[baseline]
                \node[bar, fill=red!30, minimum height=0.4cm] at (0,0) {};
                \node[bar, fill=blue!80, minimum height=1.1cm] at (0.8,0) {};
                \node[font=\tiny] at (0,-0.6) {计算40\%};
                \node[font=\tiny] at (0.8,-0.75) {显存95\%};
            \end{tikzpicture}
        };
        
        % 箭头表示资源浪费
        \draw[<->, thick, dashed, red] (1.5,0.5) -- (3.5,0.5) node[midway, above, font=\small] {资源互补但无法流动};
    \end{tikzpicture}
    \caption{Prefill与Decode实例资源利用率失衡}
    \label{fig:resource-imbalance}
\end{figure}

现有系统缺乏在实例间动态迁移计算任务或内存状态的能力，无法根据实时负载重新平衡资源。

\subsection{对异构性和动态性的适应性不足}

现有方案多假设同构的GPU集群和稳态负载，对以下实际场景缺乏有效支持：

\textbf{(1) 硬件异构性}。实际数据中心常混合部署不同型号GPU（如A100、H100、RTX 3090），其计算能力、显存容量、互联带宽差异显著。现有方法未充分考虑这些差异，导致资源碎片化严重。

\textbf{(2) 网络拓扑异构性}。GPU间通过NVLink、PCIe、InfiniBand等多种互联方式连接，带宽差异可达10倍以上。现有部署策略忽视拓扑结构，可能将频繁通信的层分配到高延迟链路上。

\textbf{(3) 负载动态性}。生产环境负载呈现重尾分布（Heavy-tailed）和突发特性，现有静态或准静态策略无法及时响应，导致服务质量波动。

针对上述局限，本文后续章节提出两种优化方案：第三章的\textbf{UELLM}聚焦于静态场景下的批处理优化与异构部署，通过输出长度预测和动态规划实现高效资源分配；第四章的\textbf{BanaServe}针对动态PD分离架构，通过层级资源迁移和全局KV Cache共享实现细粒度负载均衡。