% !TeX root = ../sustechthesis-example.tex

\begin{denotation}[3cm]

  % ===== 大写拉丁字母 =====
  \item[$A^{(j)}$]注意力权重矩阵，设备$j$上的注意力分数指数值
  \item[$B$]有效网络带宽（Gbps）
  \item[$B_{\text{net}}$]节点间有效互联带宽
  \item[$B_{sz}$]批处理大小（Batch Size）
  \item[$C_d$]设备$d$当前计算使用量
  \item[$C^{\max}_d$]设备$d$的峰值计算能力上限
  \item[$C_\ell$]每层每token的计算开销
  \item[$C_{\text{gpu}}$]GPU峰值计算能力
  \item[$CM$]当前批次综合优化指标
  \item[$D$]硬件设备节点集合，$D=\{d_1,d_2,\ldots,d_n\}$
  \item[$E$]硬件网络拓扑中节点间连接边集合
  \item[$G$]硬件网络拓扑图，$G=(D,E)$
  \item[$H$]注意力头总数
  \item[$K$]输出长度分桶数量；键矩阵（Key Matrix）
  \item[$K^{(j)}$]分配至设备$j$的键矩阵子集
  \item[$K_{\text{acc}}$]Decode实例中解码过程累积的KV Cache大小
  \item[$K_{\text{init}}$]Prefill实例的初始KV Cache分配大小
  \item[$L$]输入序列长度（token数）
  \item[$L_1$]并行计算引入的延迟系数（与SLO相关）
  \item[$L_2$]并行计算引入的延迟系数（与输出长度相关）
  \item[$L_{CM}$]当前批次中的最大SLO值
  \item[$M$]LLM显存总需求
  \item[$M_0$]进程基础内存开销
  \item[$M_d$]设备$d$当前显存使用量
  \item[$M^{\max}_d$]设备$d$的显存容量上限
  \item[$M_\ell$]每层显存占用
  \item[$N$]当前批次中的请求数量；Transformer总层数；并发请求总数
  \item[$N_m$]模块选择和开销评估的计算量
  \item[$O$]批次内最大输出长度；注意力输出矩阵
  \item[$O^{(j)}$]设备$j$上计算得到的注意力输出子集
  \item[$O_{CM}$]当前批次中的最大输出长度
  \item[$P$]Prefill实例集合
  \item[$Q$]查询矩阵（Query Matrix）；请求队列
  \item[$S$]设备分配方案，$S\subseteq D$
  \item[$S_{kv}$]单token每层KV Cache大小（字节）
  \item[$S^{kv}_\ell$]第$\ell$层对应KV Cache大小
  \item[$S^{\text{total}}_\ell$]层级迁移总数据量（权重+KV Cache）
  \item[$S^w_\ell$]第$\ell$层权重大小
  \item[$T$]KV Cache预留显存；延迟阈值
  \item[$T_{\text{attn}}$]注意力级KV Cache迁移延迟
  \item[$T_{\text{budget}}$]迁移延迟预算约束
  \item[$T_c$]缓存访问延迟
  \item[$T_d$]基础解码计算时间
  \item[$T_F$]Prefill阶段总前向计算时间
  \item[$T_{F,\text{layer}}$]每层前向计算时间
  \item[$T_{\text{KV}}$]每层KV Cache传输时间
  \item[$T_l$]批次总延迟优化目标
  \item[$T_{\text{layer}}$]层级权重迁移延迟
  \item[$T_m$]内存带宽停顿时间
  \item[$T_{\text{mem\_realloc}}$]缓冲区重分配时间
  \item[$T_o$]批次总输出长度优化目标
  \item[$T_p$]Prefill计算时间
  \item[$T_q$]Decode前的排队延迟
  \item[$T_{\text{sync}}$]迁移同步开销
  \item[$T_{\text{TPOT}}$]每输出token时间（Time Per Output Token）
  \item[$T_{\text{TTFT}}$]首token延迟（Time To First Token）
  \item[$T_x$]KV Cache传输时间（加载+获取）
  \item[$U_{\text{avg}}$]集群平均GPU利用率
  \item[$U_d$]设备$d$的归一化综合利用率
  \item[$U_p$]Prefill实例平均利用率
  \item[$U_{p_i}$]Prefill实例$p_i$的归一化负载
  \item[$V$]值矩阵（Value Matrix）
  \item[$V^{(j)}$]分配至设备$j$的值矩阵子集
  \item[$W_\ell$]第$\ell$层的模型权重矩阵

  % ===== 小写拉丁字母 =====
  \item[$b$]批次中的请求数量
  \item[$d$]注意力头维度
  \item[$d_{\text{head}}$]每个注意力头的维度
  \item[$d_{\text{model}}$]模型隐藏层维度
  \item[$h_{kv}$]GQA下KV头数量
  \item[$h_{\text{total}}$]总注意力头数量
  \item[$m$]每层所需显存（$M/\text{Layer}(M)$）
  \item[$n_d$]Decode实例分配的Transformer层数
  \item[$n_p$]Prefill实例分配的Transformer层数
  \item[$p$]性能-时间关系调节系数
  \item[$q_i$]第$i$个推理请求
  \item[$r$]平均前缀缓存命中率

  % ===== 缩略语 =====
  \item[AI]人工智能（Artificial Intelligence）
  \item[API]应用程序接口（Application Programming Interface）
  \item[BF16]Brain Float 16位浮点数格式
  \item[CPU]中央处理器（Central Processing Unit）
  \item[CUDA]统一计算设备架构（Compute Unified Device Architecture）
  \item[DistServe]预填充-解码分离推理系统\cite{zhong2024distserve}
  \item[DP]动态规划（Dynamic Programming）
  \item[FIFO]先进先出调度策略（First-In-First-Out）
  \item[FP16]16位浮点数格式（Float Point 16）
  \item[GPU]图形处理单元（Graphics Processing Unit）
  \item[GQA]分组查询注意力机制（Grouped Query Attention）
  \item[HBM]高带宽显存（High Bandwidth Memory）
  \item[HE]高效利用率模式（High-Efficiency Mode）
  \item[HELR]高效低延迟资源分配算法（High-Efficiency Low-Latency Resource Allocation）
  \item[HFT]Hugging Face Transformers推理框架
  \item[INT4]4位整数量化格式
  \item[INT8]8位整数量化格式
  \item[KV Cache]键值缓存（Key-Value Cache）
  \item[LJF]长作业优先调度（Longest Job First）
  \item[LLM]大语言模型（Large Language Model）
  \item[LongBench]长文本理解基准测试数据集\cite{longbench}
  \item[LoRA]低秩自适应微调（Low-Rank Adaptation）
  \item[LR]低延迟优先模式（Low-Latency Mode）
  \item[MDP]马尔可夫决策过程（Markov Decision Process）
  \item[MLaaS]机器学习即服务（Machine Learning as a Service）
  \item[MPS]多进程服务（Multi-Process Service）
  \item[MIG]多实例GPU（Multi-Instance GPU）
  \item[NVLink]NVIDIA高速GPU互联技术
  \item[ODBS]输出长度驱动的动态批处理调度算法（Output-Driven Batch Scheduler）
  \item[OOM]显存溢出（Out-of-Memory）
  \item[PCIe]高速串行计算机扩展总线标准（Peripheral Component Interconnect Express）
  \item[PD分离]预填充-解码分离架构（Prefill-Decode Disaggregation）
  \item[PPO]近端策略优化算法（Proximal Policy Optimization）
  \item[QoS]服务质量（Quality of Service）
  \item[QLoRA]量化低秩自适应微调（Quantized Low-Rank Adaptation）
  \item[RAG]检索增强生成（Retrieval-Augmented Generation）
  \item[RDMA]远程直接内存访问（Remote Direct Memory Access）
  \item[RPS]每秒请求数（Requests Per Second）
  \item[S3]基于输出长度感知的调度框架\cite{jin2023s3}
  \item[SAC]软演员-评论家算法（Soft Actor-Critic）
  \item[SGLang]结构化语言模型程序执行框架\cite{zheng2024sglang}
  \item[SJF]短作业优先调度（Shortest Job First）
  \item[SLO]服务等级目标（Service Level Objective）
  \item[SLO-DBS]SLO感知的动态批处理调度算法（SLO-aware Dynamic Batch Scheduler）
  \item[SLO-ODBS]SLO感知与输出长度驱动的动态批处理调度算法（SLO and Output-Driven Dynamic Batch Scheduler）
  \item[SSD]固态硬盘（Solid State Drive）
  \item[TPOT]每输出token时间（Time Per Output Token）
  \item[TTFT]首token延迟（Time To First Token）
  \item[UA]UELLM-all，同时采用HELR和SLO-ODBS的完整版本
  \item[UB]UELLM-batch，仅使用SLO-ODBS批处理调度算法的版本
  \item[UD]UELLM-deploy，仅使用HELR模型部署算法的版本
  \item[UELLM]统一高效大语言模型推理服务框架（Unified and Efficient LLM Inference Serving）
  \item[vLLM]基于PagedAttention的高效LLM推理系统\cite{kwon2023efficient}
  \item[WAN]广域网（Wide Area Network）

  % ===== 希腊字母 =====
  \item[$\alpha$]联合优化目标中资源利用率的权重系数
  \item[$\beta$]联合优化目标中延迟的权重系数
  \item[$\gamma$]联合优化目标中吞吐量的权重系数
  \item[$\delta$]负载不均衡阈值
  \item[$\delta_L$]负载感知调度中的负载阈值
  \item[$\delta_\uparrow$]迁移触发的上升阈值（滞后控制）
  \item[$\delta_\downarrow$]迁移恢复的下降阈值（滞后控制）
  \item[$\Delta$]过载设备与欠载设备之间的瞬时负载差
  \item[$\eta_{\text{static}}$]静态批处理下GPU有效利用率
  \item[$\Theta$]系统有效吞吐量（tokens/s或requests/s）
  \item[$\kappa$]迁移收益-开销比阈值（效率比$\rho$）
  \item[$\mathcal{B}$]输出长度分桶集合
  \item[$\mathcal{D}$]Decode GPU实例集合
  \item[$\mathcal{K}_\ell$]第$\ell$层的KV Cache
  \item[$\mathcal{P}$]Prefill GPU实例集合
  \item[$\ell^{(j)}$]设备$j$上注意力归一化因子（softmax分母）
  \item[$\pi$]模块迁移方案
  \item[$\pi^*$]最优模块迁移方案
  \item[$\rho$]迁移效率比阈值（可配置参数）
  \item[$w_1$]SLO-ODBS算法中总延迟的权重系数
  \item[$w_2$]SLO-ODBS算法中总输出长度的权重系数

\end{denotation}
